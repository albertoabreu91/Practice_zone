{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset css and font defaults in:\r\n",
      "/Users/abreualberto91/.jupyter/custom &\r\n",
      "/Users/abreualberto91/Library/Jupyter/nbextensions\r\n"
     ]
    }
   ],
   "source": [
    "!jt -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abreualberto91/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counts with bag-of-words\n",
    "\n",
    "- Basic methods of fiding topics in a text\n",
    "\n",
    "steps:\n",
    "\n",
    "1- Need first to create token using tokenizations\n",
    "\n",
    "2- Count all the token you came up with\n",
    "\n",
    "The theory says that the more frequent a word or token is, the most central or important it might be to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n",
      "[('The', 3), ('cat', 3)]\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"The cat is in the box. The cat likes the box. \n",
    "                 The box is over the cat.\"\"\"\n",
    "\n",
    "counter = Counter(word_tokenize(string))\n",
    "print(counter)\n",
    "\n",
    "print(counter.most_common(2)) #check that box also is mentioned 3 times so be carefull with most_common function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':', 40), ('soldier', 24), ('#', 24), (',', 23), ('1', 19), ('?', 19), ('.', 18), ('arthur', 17), ('!', 17), ('the', 17)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(scene_one)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"why_preprocess.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abreualberto91/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abreualberto91/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat is in the box. The cat likes the box. \\n                 The box is over the cat.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'likes', 'the', 'box', 'the', 'box', 'is', 'over', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "#will return True if the string only has alphabetical characters\n",
    "tokens = [word for word in word_tokenize(string.lower()) if word.isalpha()] \n",
    "print(tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n"
     ]
    }
   ],
   "source": [
    "no_stops_words = [token for token in tokens if token not in stopwords.words('english')]\n",
    "print(no_stops_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3), ('likes', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops_words).most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soldier', 24), ('arthur', 17), ('clop', 9), ('coconut', 8), ('swallow', 8), ('could', 5), ('camelot', 4), ('well', 4), ('ridden', 3), ('land', 3)]\n"
     ]
    }
   ],
   "source": [
    "#Retaing only alphabetical tokens\n",
    "\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(Counter(bow).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a gensim dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "'I really liked the movie!',\n",
    "'Awesome action scenes, but boring characters.',\n",
    "'The movie was awful! I hate alien films.',\n",
    "'Space is cool! I liked the movie.',\n",
    "'More space films, please!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'liked', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['movie', 'spaceship', 'aliens'],\n",
       " ['really', 'liked', 'movie'],\n",
       " ['awesome', 'action', 'scenes', 'boring', 'characters'],\n",
       " ['movie', 'awful', 'hate', 'alien', 'films'],\n",
       " ['space', 'cool', 'liked', 'movie'],\n",
       " ['space', 'films', 'please']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for lst in tokenized_doc:\n",
    "    for i in lst:\n",
    "        if (i.isalpha() == False) | (i in stopwords.words('english')):\n",
    "            lst.remove(i)\n",
    "        else:\n",
    "            wordnet_lemmatizer.lemmatize(i)\n",
    "        \n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Gensim dictionary\n",
    "\n",
    "- This will create a mapping with and ID for each Token\n",
    "\n",
    "We can now represent whole documents using just a list of their token ids and how often thoose token appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliens': 0,\n",
       " 'movie': 1,\n",
       " 'spaceship': 2,\n",
       " 'liked': 3,\n",
       " 'really': 4,\n",
       " 'action': 5,\n",
       " 'awesome': 6,\n",
       " 'boring': 7,\n",
       " 'characters': 8,\n",
       " 'scenes': 9,\n",
       " 'alien': 10,\n",
       " 'awful': 11,\n",
       " 'films': 12,\n",
       " 'hate': 13,\n",
       " 'cool': 14,\n",
       " 'space': 15,\n",
       " 'please': 16}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gensim Corpus\n",
    "\n",
    "Gensim uses a simple bag-of-words model which transforms each document into a bag of words using the tokend ids and the frequency of each token in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(1, 1), (3, 1), (4, 1)],\n",
       " [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n",
       " [(1, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(1, 1), (3, 1), (14, 1), (15, 1)],\n",
       " [(12, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_doc]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the Gensim corpus is a list of list, each list item representing one document.\n",
    "\n",
    "- the first element of the list represents the token ID and the second one represent the frequency in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gensim_model.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are word vectors?\n",
    "\n",
    "##### What are word vectors and how do they help with NLP?\n",
    "\n",
    "- Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gensim_bow.png\" width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [['uses',\n",
    "  'file',\n",
    "  'operating',\n",
    "  'system',\n",
    "  'placement',\n",
    "  'software',\n",
    "  '.svg|thumb|upright|a',\n",
    "  'diagram',\n",
    "  'showing',\n",
    "  'user',\n",
    "  'computing',\n",
    "  '|user',\n",
    "  'interacts',\n",
    "  'application',\n",
    "  'software',\n",
    "  'typical',\n",
    "  'desktop',\n",
    "  'computer.the',\n",
    "  'application',\n",
    "  'software',\n",
    "  'layer',\n",
    "  'interfaces',\n",
    "  'operating',\n",
    "  'system',\n",
    "  'turn',\n",
    "  'communicates',\n",
    "  'personal',\n",
    "  'computer',\n",
    "  'hardware|hardware',\n",
    "  'arrows',\n",
    "  'indicate',\n",
    "  'information',\n",
    "  'flow',\n",
    "  \"''\",\n",
    "  'computer',\n",
    "  'software',\n",
    "  \"''\",\n",
    "  'simply',\n",
    "  \"''\",\n",
    "  'software',\n",
    "  \"''\",\n",
    "  'part',\n",
    "  'computer',\n",
    "  'system',\n",
    "  'consists',\n",
    "  'data',\n",
    "  'computing',\n",
    "  '|data',\n",
    "  'computer',\n",
    "  'instructions',\n",
    "  'contrast',\n",
    "  'computer',\n",
    "  'hardware|physical',\n",
    "  'hardware',\n",
    "  'system',\n",
    "  'built',\n",
    "  'computer',\n",
    "  'science',\n",
    "  'software',\n",
    "  'engineering',\n",
    "  'computer',\n",
    "  'software',\n",
    "  'information',\n",
    "  'processed',\n",
    "  'computer',\n",
    "  'systems',\n",
    "  'computer',\n",
    "  'program|programs',\n",
    "  'data',\n",
    "  'computer',\n",
    "  'software',\n",
    "  'includes',\n",
    "  'computer',\n",
    "  'programs',\n",
    "  'library',\n",
    "  'computing',\n",
    "  '|libraries',\n",
    "  'related',\n",
    "  'non-executable',\n",
    "  'data',\n",
    "  'computing',\n",
    "  '|data',\n",
    "  'software',\n",
    "  'documentation|online',\n",
    "  'documentation',\n",
    "  'digital',\n",
    "  'media',\n",
    "  'computer',\n",
    "  'hardware',\n",
    "  'software',\n",
    "  'require',\n",
    "  'neither',\n",
    "  'realistically',\n",
    "  'used',\n",
    "  'lowest',\n",
    "  'level',\n",
    "  'executable',\n",
    "  'code',\n",
    "  'consists',\n",
    "  'machine',\n",
    "  'code|machine',\n",
    "  'language',\n",
    "  'instructions',\n",
    "  'specific',\n",
    "  'individual',\n",
    "  'microprocessor|processor—typically',\n",
    "  'central',\n",
    "  'processing',\n",
    "  'unit',\n",
    "  'cpu',\n",
    "  'machine',\n",
    "  'language',\n",
    "  'consists',\n",
    "  'groups',\n",
    "  'binary',\n",
    "  'numbers|binary',\n",
    "  'values',\n",
    "  'signifying',\n",
    "  'processor',\n",
    "  'instructions',\n",
    "  'change',\n",
    "  'state',\n",
    "  'computer',\n",
    "  'preceding',\n",
    "  'state',\n",
    "  'example',\n",
    "  'instruction',\n",
    "  'may',\n",
    "  'change',\n",
    "  'value',\n",
    "  'stored',\n",
    "  'particular',\n",
    "  'storage',\n",
    "  'location',\n",
    "  'computer—an',\n",
    "  'effect',\n",
    "  'directly',\n",
    "  'observable',\n",
    "  'user',\n",
    "  'instruction',\n",
    "  'may',\n",
    "  'also',\n",
    "  'indirectly',\n",
    "  'cause',\n",
    "  'something',\n",
    "  'appear',\n",
    "  'display',\n",
    "  'computer',\n",
    "  'system—a',\n",
    "  'state',\n",
    "  'change',\n",
    "  'visible',\n",
    "  'user',\n",
    "  'processor',\n",
    "  'carries',\n",
    "  'instructions',\n",
    "  'order',\n",
    "  'provided',\n",
    "  'unless',\n",
    "  'instructed',\n",
    "  'branch',\n",
    "  'instruction|',\n",
    "  \"''\",\n",
    "  'jump']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lst in articles:\n",
    "    for i in lst:\n",
    "        if (i.isalpha() == False) | (i in stopwords.words('english')):\n",
    "            lst.remove(i)\n",
    "        else:\n",
    "            wordnet_lemmatizer.lemmatize(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 14\n",
      "software 10\n",
      "computing 4\n",
      "instructions 4\n",
      "system 4\n",
      "defaultdict(<class 'int'>, {0: 1, 1: 1, 2: 1, 3: 2, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 3, 12: 1, 13: 1, 14: 14, 15: 4, 16: 3, 17: 1, 18: 1, 19: 3, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 2, 34: 1, 35: 1, 36: 1, 37: 1, 38: 2, 39: 1, 40: 2, 41: 4, 42: 1, 43: 1, 44: 1, 45: 2, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 2, 52: 2, 53: 1, 54: 1, 55: 1, 56: 2, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 2, 66: 1, 67: 1, 68: 1, 69: 1, 70: 1, 71: 1, 72: 1, 73: 1, 74: 1, 75: 10, 76: 1, 77: 1, 78: 3, 79: 1, 80: 1, 81: 4, 82: 1, 83: 1, 84: 1, 85: 1, 86: 1, 87: 1, 88: 3, 89: 1, 90: 1, 91: 1, 92: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[0]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "print(total_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 14),\n",
       " (75, 10),\n",
       " (15, 4),\n",
       " (41, 4),\n",
       " (81, 4),\n",
       " (11, 3),\n",
       " (16, 3),\n",
       " (19, 3),\n",
       " (78, 3),\n",
       " (88, 3),\n",
       " (3, 2),\n",
       " (33, 2),\n",
       " (38, 2),\n",
       " (40, 2),\n",
       " (45, 2),\n",
       " (51, 2),\n",
       " (52, 2),\n",
       " (56, 2),\n",
       " (65, 2),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (39, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (53, 1),\n",
       " (54, 1),\n",
       " (55, 1),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 1),\n",
       " (87, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "sorted_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 14\n",
      "software 10\n",
      "computing 4\n",
      "instructions 4\n",
      "system 4\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
